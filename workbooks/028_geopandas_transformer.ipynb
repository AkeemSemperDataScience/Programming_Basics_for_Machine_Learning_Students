{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers and Geopandas\n",
    "\n",
    "<b>Note:</b> the sklearn transformer and the neural network transformer we'll mention later, and probably several other things named 'transformer' are all different things. \n",
    "\n",
    "## What is an SKLearn Transformer?\n",
    "\n",
    "SKlearn transformers are classes that implement the `fit` and `transform` methods. They are used to preprocess data before feeding it to a model. In normal English, they are a step in data preparation pipelines that apply some type of clean-up step to the data. We use existing transformers all the time to impute, scale, or encode data.\n",
    "\n",
    "We can also write our own custom transformers by extending the `BaseEstimator` and `TransformerMixin` classes from the `sklearn.base` module. This is useful when we need to apply some custom transformation that is not available in the existing transformers. As long as we provide the needed functionality, our custom transformer can be used in the same way as the built-in transformers.\n",
    "\n",
    "## New - Pandas Got Better\n",
    "\n",
    "Recently, I think in 2023, the usability of the pipeline transformers was improved a bit by allowing them to work with pandas dataframes, not only arrays. Dataframes are much easier to use as a human, so this will make steps using and tailoring transformers easier. In the examples later in Machine Learning, we assume we need an array, so we make arrays before starting the pipeline. We now have more flexibility to keep the data in that dataframe format longer, so we can handle it through the pipeline process with more ease. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's a Pipeline?\n",
    "\n",
    "A pipeline in sklearn is a sequence of steps that are applied to the data. In data science, we often need to load a large amount of data, and apply processing steps to that data in bulk to do things like impute, scale, or encode it. The pipeline is a way to automate this process so we can treat it as a group of steps, not a whole bunch of individual actions we need to manage bit by bit. \n",
    "\n",
    "In most cases, we will load our data, send it through the pipleline, and the output of that will go to the predictive modelling algorithm. All the preprocessing steps are done in the pipeline, so we can just focus on the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Pipeline Example\n",
    "\n",
    "Here's a simple example of a pipeline - any data fed through this pipeline will have two steps applied to it - first the imputation (filling in blanks), then the scaling (making sure all the data is on the same scale). Each of these steps is a transformer - an object that we can make by extending some classes. These steps can be literally anything that we can imagine, as long as we meet the expectations of what a transformer needs (fit and transform methods) and we accept and return the data in the correct format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "pipe = pipeline.Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the make_pipeline function to make a pipeline. It really doesn't matter, I personally never use make_pipeline, but it's there if you want it. The other way has the step name part built in, so that's a bit better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "pipe_for_fake_chumps = make_pipeline(\n",
    "    SimpleImputer(strategy=\"mean\"),\n",
    "    StandardScaler()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indentation\n",
    "\n",
    "This is something of a personal preference, but I find it far easier to read for these types of things where the items are lined up as they are above. VS Code will usually do this pretty well by default, but it really is much more readable than listing them all in a line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/titanic_train.csv\")\n",
    "cat_cols = df.select_dtypes(include=['object']).columns\n",
    "df.drop(cat_cols, axis=1, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_after = pipe.fit_transform(df)\n",
    "pd.DataFrame(pd_after, columns=df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "\n",
    "What happened here? The original data went through the two steps, each of which applied some transformation that changed the data:\n",
    "<ul>\n",
    "<li> The imputer found each empty value, and replaced it with the mean of the column. </li>\n",
    "<li> The scaler did a two step scaling process: </li>\n",
    "    <ul>\n",
    "    <li> The 'fit' part allowed the scaler to learn the mean and standard deviation of the data. </li>\n",
    "    <li> The 'transform' part applied the scaling to the data - setting the values to be \"standardized\" - mean of 0 and standard deviation of 1. </li>\n",
    "    <li> <b> The details of this should be covered in stats, we're just worried about the step here. </b> </li>\n",
    "    </ul>\n",
    "</ul>\n",
    "\n",
    "### Pipeline Details\n",
    "\n",
    "The pipeline is a pretty simple concept at its core - it is a series of steps where data goes in one end, each step does whatever it does, and the result comes out the end. Each pipeline step has two parts defined:\n",
    "<ul>\n",
    "<li> Name - this is how we refer to the step. When making more complex pipelines, we may need this. We can also grab a step by name. </li>\n",
    "<li> Action - this is the transformer that is applied to the data. Any configuration goes in the constructor call here. </li>\n",
    "</ul>\n",
    "\n",
    "Once created, it does those steps to that data in that order. This is really useful in a real life scenario where we likely have lots of data either in an incoming flow or regular batches to be processed. If we make a pipeline, then all the processing is handled there - we can capture the data in its original format and trust that our pipeline will do the right thing to it.\n",
    "\n",
    "<b>Note:</b> there are ways to have a smarter pipeline, that can do things like process part of the data in one way and part in another (like discreet and continuous data) - that uses the same pipeline framework, but with a couple of slightly more complex tools. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.named_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise - Make a Simple Pipeline\n",
    "\n",
    "Make a pipeline for the titanic data. Try to have it do the following things:\n",
    "<ul>\n",
    "<li> Fill in missing values with the median value of that column. (Impute) </li>\n",
    "<li> Scale the data so all the columns are on the same scale. (Use Min-max scaler) </li>\n",
    "</ul>\n",
    "\n",
    "If you're feeling ok here, try to incorporate some other steps. Most work with little to no modifications to the code you need to write. If you search for \"sklearn pipeline\" the documentation page has a link to a User Guide, which is a pretty good article with examples, there are some different transformers in there that you can try out. You can also just google \"sklearn pipeline transformers\" and find one that you can try. For the most part, these are all related to preparing data for ML modeling, so the details aren't something we have worried about yet, but the act of using the transformers is identical no matter what they actually do. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Do I Care?\n",
    "\n",
    "For the moment, we don't need to care about the details of those steps, but we do want to use those mechanics to do our work. The steps that the pipeline does can be anything we want, so if we have some spatial cleanup, we can make a transformer to do that, and add it to the pipeline!\n",
    "\n",
    "In machine learning later on, we will use variations of these pipelines to load most of the data we use. It is even more seamless there, we load the data, then the pipeline feeds the output directly into the modelling algorithm - that wall of numbers version above is hidden from us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline Creation Options\n",
    "\n",
    "There are a few ways to make a pipeline, and in general pipeline components can be connected together and swapped out as needed. This leans heavily on the duck typing pattern of thought we've been getting used to - we can swap things around, as long as it does what is required of it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Can Transformers Do?\n",
    "\n",
    "Pretty much anything you want, some common examples include:\n",
    "<ul>\n",
    "<li> Imputing missing values </li>\n",
    "<li> Scaling numerical features </li>\n",
    "<li> Encoding categorical features </li>\n",
    "<li> Extracting features from text </li>\n",
    "<li> Reducing dimensionality </li>\n",
    "</ul>\n",
    "\n",
    "Basically anything we can express in a statement of \"change the data in this way\" can be a transformer.\n",
    "\n",
    "### And How Do They Do It?\n",
    "\n",
    "A transfomer is a pretty simple concept. It has two main methods:\n",
    "<ul>\n",
    "<li> `fit` - This method is used to learn the parameters of the transformation. For example, if we are scaling numerical features, the `fit` method will calculate the mean and standard deviation of each feature. </li>\n",
    "    <ul>\n",
    "    <li> In many cases, the `fit` method does nothing, it just returns 'self', but it is still required to be there. </li>\n",
    "    <li> If we were imputing and inserting 'median' for missing values, this step would calculate the median of each feature. </li>\n",
    "    </ul>\n",
    "<li> `transform` - This method is used to apply the transformation to the data. For example, if we are scaling numerical features, the `transform` method will subtract the mean and divide by the standard deviation. </li>\n",
    "</ul>\n",
    "\n",
    "Each of these methods is automatically called by the sklearn pipeline when we insert them as steps. If we have any configuration, such as the number of features to extract or the method of scaling, we can pass these as arguments to the transformer's `__init__` method.\n",
    "\n",
    "### Making a Customized Transformer\n",
    "\n",
    "As long as we follow these expectations, we can make a pipeline step doing almost anything we want. We need to accept values in an expected format - a 2D array or dataframe, and return the values in that same correct format - the details can vary though, we can move, add, delete or change values, rows, and columns as we need. Each manipulation in the transform steps of transformers is basically a prescribed set of steps we could do to any spreadsheet of data. \n",
    "\n",
    "<b>Note:</b> the X copy thing isn't required, it is there as a safety thing to prevent accidental side effects. It's a generally good (or more accurately, safe) idea, but not a requirement. There's at least one solution below without it. The biggest risk here would be if you were to do something that changed the data in place, and then tried to use the original data later - for example, if your transformer has a sort, you don't want to accidentally sort the original data and then try to use it later. In normal usage in a pipeline, this isn't normally something that comes up. It is possible, and can be very confusing if it does. \n",
    "\n",
    "### Transformer TL;DR\n",
    "\n",
    "So, we can make a transformer to do anything we want, we just need to follow some constraints:\n",
    "<ul>\n",
    "<li> We need to extend the BaseEstimator and TransformerMixin classes. </li>\n",
    "<li> We need to implement the fit and transform methods. </li>\n",
    "<li> We need to accept and return the data in the correct format - a 2D table in either DF or array. </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self, X, y=None):\n",
    "        return self  # The fit method typically does nothing for transformers\n",
    "        # This is mainly used when there is a 'configuration' \n",
    "        # step that needs to be done before the transformation\n",
    "        # For example, when scaling data with standardization, \n",
    "        # we'd need the mean and std of the data - that's calculated here.\n",
    "    def transform(self, X):\n",
    "        # Your transformation logic goes here\n",
    "        X_transformed = X.copy()  # Copy the input DataFrame to avoid modifying the original\n",
    "        ############## Your code here ##############\n",
    "        return X_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Custom Transformers\n",
    "\n",
    "Adding a custom transformer to a pipeline is identical to using a premade one - that's one of the big benefits of the interchangeability of objects we have with python, duck typing, and inheritance - we only need to provide a tiny portion of the functionality that differs, not learn and reimplement the entire thing!\n",
    "\n",
    "We must conform strictly to the input and output format, as well as the required methods, but beyond that we can do whatever we want. For this example below, we are adding two steps - the area calculator and the custom one. Each step gets a name to refer to it from here on out, and each init call can contain any arguments that we need or want to provide for configuration. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise - Make a Custom Transformer\n",
    "\n",
    "Make a custom transformer that adds a column to the data. The column should be the product of x and y.\n",
    "\n",
    "If that works, add an optional parameter that allows the user to add a constant that defaults to 1, this constant should be multiplied by the product of x and y.\n",
    "\n",
    "If all that works, add the ability to specify the column names for x and y, and name the output column. \n",
    "\n",
    "<b>Use your transformer by calling fit_transform, as well as in a pipeline.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "\n",
    "X, y = make_circles(n_samples=1000, factor=0.5, noise=0.1)\n",
    "df_xy = pd.DataFrame(X, columns=[\"x\", \"y\"])\n",
    "df_xy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipe it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geopandas and Pipelines\n",
    "\n",
    "Geopandas is built \"on top of\" pandas, adding the ability to handle geospatial data to the already powerful pandas data manipulation library. We can use geopandas dataframes exactly as we would use a regular pandas dataframe, but with the added ability to handle geospatial data. Since geopandas dataframes are just pandas dataframes with some extra functionality, we can use them in sklearn pipelines as well.\n",
    "\n",
    "There's little, if any, difference in when we make data a geodataframe. As a rough rule of thumb, if there's lots of manipulation to do with data, I'll load it as a pandas dataframe and make a geodataframe with that data; if there is not much processing, I'll read it with GeoPandas directly and set the geometry right up front. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpd_sample = gpd.read_file( \"../data/03_city_edmonton_assessment_sample.csv\", geometry=\"geometry\")\n",
    "gpd_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_sample = pd.read_csv(\"../data/03_city_edmonton_assessment_sample.csv\")\n",
    "pandas_sample = gpd.GeoDataFrame(pandas_sample, geometry=gpd.points_from_xy(pandas_sample[\"Latitude\"], pandas_sample[\"Longitude\"]))\n",
    "pandas_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Realistic(-ish) Examples\n",
    "\n",
    "Here are a couple of simple examples. The first adds an area column, the second generates points from lat/lon and places that into the geometry column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class areaGenerator(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X[\"area\"] = self.area\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointGenerator(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, lon, lat):\n",
    "        self.lon = lon\n",
    "        self.lat = lat\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X_transformed = X.copy()\n",
    "        X_transformed[\"geometry\"] = gpd.points_from_xy(X_transformed[self.lon], X_transformed[self.lat])\n",
    "        return X_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example - Spatial Join Transformer\n",
    "\n",
    "In this example, we can use a spatial join to connect two datasets based on their location. Specifically, we can use the point position of items in our data to connect with the area polygons in a spatial dataset, and get the neighborhood label from that spatial dataset. To make this work, we'll need a few parts:\n",
    "<ul>\n",
    "<li> A geopandas dataframe with the spatial data. This is like a setting or configuration step, so it will be in the constructor. </li>\n",
    "<li> A transformer that can take the 'regular' data we're using, perform the spatial join, and add the result in a new column. </li>\n",
    "</ul>\n",
    "\n",
    "As long as our input and outputs match the sklearn transformer format, we can use this in a pipeline just like any other transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Spatial_Joiner( BaseEstimator, TransformerMixin):\n",
    "    # Still must complete. \n",
    "    def __init__(self, other):\n",
    "        self.other = other\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X = gpd.sjoin(X, self.other, how=\"left\", op=\"contains\")\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare Data\n",
    "\n",
    "I'll prepare the data here. The main thing is to ensure that geometry has properly captured the spatial data from the file. In this example, each column requires a little processing to get it into the right format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "booze = gpd.read_file('../data/Alcohol_Sales_Licences.csv', crs=6933)\n",
    "booze['geometry'] = gpd.points_from_xy(booze['Longitude'], booze['Latitude'])\n",
    "booze.set_geometry('geometry', inplace=True, crs=6933)\n",
    "\n",
    "hoods = gpd.read_file('../data/Neighbourhood_Boundaries.csv')\n",
    "hoods[\"geometry\"] = gpd.GeoSeries.from_wkt(hoods[\"geom\"])\n",
    "hoods.set_geometry(\"geometry\", inplace=True, crs=6933)\n",
    "hoods.drop(\"geom\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joinPipe = pipeline.Pipeline([\n",
    "    (\"joiner\", Spatial_Joiner(other=booze))\n",
    "])\n",
    "tran_join_result = joinPipe.fit_transform(hoods)\n",
    "tran_join_result.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geospatial Transformations\n",
    "\n",
    "With geospatial data, particularly for the things you're likely to be doing, these transformers and piplines can be used to make tools to process the data automatically into a format that provides what you need for analysis. We can create a pipeline that takes in raw data and outputs it in some format that we know we want - the steps to do those transformations are the transformers in the pipeline. Once built, we can process any new data with no additional effort by just running it through the pipeline - this means that we'd never do something like manually manipulate data in Excel or something like that, we'd always use the pipeline to do it for us.\n",
    "\n",
    "For your applications, you'll get some data and the format you need it in might vary, or you likely might need more than one format. For example, if you were displaying some data in Tableau or Power BI, you can take some raw data, run it through a pipeline that calculates whatever values need to be displayed, then output that as a datasource for your visualization. The transformer step might do all kinds of stuff like calculate distance, do spatial joins to get region labels, calculate area, etc... This data can then be fed to the visualization tool, and it can make pretty pictures without having to manipulate data there. There can be multiple pipelines (or outputs) that each prepares one central source of data, automatically, for different purposes.\n",
    "\n",
    "If this sounds similar to some ETL stuff you talked about in the database classes, that's because it is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddAreaCalculation( BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, column_name):\n",
    "        self.column_name = column_name\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X_transformed = X.copy()\n",
    "        X_transformed[self.column_name] = X_transformed[\"geometry\"].area\n",
    "        return X_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise - Do That With Transformers\n",
    "\n",
    "The processing steps above can be replaced with a pipeline containing custom steps in transformers. For the booze, we want to create a point from the lat/lon columns and set that answer into the geometry column. For the area, we want to add an area column.\n",
    "\n",
    "<b>Hint:</b> we can chain transformers together almost any way we want, they can be connected like pipes in almost any configuration. The easy way to do this might not be the most intuitive... \n",
    "\n",
    "![Pipes](../images/pipe.jpg \"Pipes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "booze2 = gpd.read_file('../data/Alcohol_Sales_Licences.csv')\n",
    "hoods2 = gpd.read_file('../data/Neighbourhood_Boundaries.csv')\n",
    "\n",
    "#Make Pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformative Thoughts\n",
    "\n",
    "As we make these transformers, there are a few things that we might want to consider more than we would in other scenarios:\n",
    "<ul>\n",
    "<li> Speed - these transfomrations may be run on all data, so efficiency might matter. </li>\n",
    "    <ul>\n",
    "    <li> We should avoid things that are obviously slow, like nested loops. </li>\n",
    "    <li> Vectorization is a good goal - like np operations on arrays. </li>\n",
    "    </ul>\n",
    "<li> Safety - we should be careful to avoid side effects. </li>\n",
    "    <ul>\n",
    "    <li> We should avoid changing the input data in place as it introduces risk. Unlikely, but possible. </li>\n",
    "    <li> If we make a transformer, we have 0 idea where it'll be used, so safety first. </li>\n",
    "    <li> It isn't relevant right now, but it isn't uncommon for us to process some data and send it to a model, then use that data for something else like visualizations or another model. We don't want accidental changes, that are invisible, to the data behind our backs. In most cases, this won't be a real issue, but it is a very hard problem to trouble shoot if it does happen. </li>\n",
    "    </ul>\n",
    "<li> Flexibility - we should make sure that the transformer can handle a variety of data. By defult pipelines can work with data in arrays and dataframs in a way that is more-or-less seamless. We want the same behaviour in most cases. This usually isn't that big of a stretch, most other functions that we might need will work with different data types already, so as long as we don't introduce some object-specific action, we should be ok. </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise - Make Some Transformers\n",
    "\n",
    "For this exercise, create a transformer that adds a new column representing the distance between the two locations in the dataframe. The transformer should take the two columns containing the latitude and longitude of the two locations as input and add a new column with the distance between them.\n",
    "\n",
    "Create a pipeline with transformers that:\n",
    "<ul>\n",
    "<li> Adds a new column with the distance to a set point. </li>\n",
    "<li> Adds a new column with the distance to the nearest point in another dataset. </li>\n",
    "<li> Adds a new column 'geometry' that is the point geometry, from lat and lon. </li>\n",
    "<li> Add a transformer that filters things that are within a set distance of a point. </li>\n",
    "</ul>\n",
    "\n",
    "<b>Editor's Note:</b> I wrote these first, and they ended up being a bit redundant. I just didn't delete these prompts, but there's some overlap with the stuff above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you know what to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_play = pd.read_csv(\"../data/playgrounds.csv\")\n",
    "ex_play.drop(columns=[\"Location\", \"Geometry Point\"], inplace=True)\n",
    "ex_play.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set point test\n",
    "nait_lat = 53.57030\n",
    "nait_long = -113.50087\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
