{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers and Geopandas\n",
    "\n",
    "<b>Note:</b> the sklearn transformer and the neural network transformer we'll mention later, and probably several other things named 'transformer' are all different things. \n",
    "\n",
    "## What is an SKLearn Transformer?\n",
    "\n",
    "SKlearn transformers are classes that implement the `fit` and `transform` methods. They are used to preprocess data before feeding it to a model. In normal English, they are a step in data preparation pipelines that apply some type of clean-up step to the data. We use existing transformers all the time to impute, scale, or encode data.\n",
    "\n",
    "We can also write our own custom transformers by extending the `BaseEstimator` and `TransformerMixin` classes from the `sklearn.base` module. This is useful when we need to apply some custom transformation that is not available in the existing transformers. As long as we provide the needed functionality, our custom transformer can be used in the same way as the built-in transformers.\n",
    "\n",
    "## New - Pandas Got Better\n",
    "\n",
    "Recently, I think in 2023, the usability of the pipeline transformers was improved a bit by allowing them to work with pandas dataframes, not only arrays. Dataframes are much easier to use as a human, so this will make steps using and tailoring transformers easier. In the examples later in Machine Learning, we assume we need an array, so we make arrays before starting the pipeline. We now have more flexibility to keep the data in that dataframe format longer, so we can handle it through the pipeline process with more ease. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's a Pipeline?\n",
    "\n",
    "A pipeline in sklearn is a sequence of steps that are applied to the data. In data science, we often need to load a large amount of data, and apply processing steps to that data in bulk to do things like impute, scale, or encode it. The pipeline is a way to automate this process so we can treat it as a group of steps, not a whole bunch of individual actions we need to manage bit by bit. \n",
    "\n",
    "In most cases, we will load our data, send it through the pipleline, and the output of that will go to the predictive modelling algorithm. All the preprocessing steps are done in the pipeline, so we can just focus on the model.\n",
    "\n",
    "### Simple Pipeline Example\n",
    "\n",
    "Here's a simple example of a pipeline - any data fed through this pipeline will have two steps applied to it - first the imputation (filling in blanks), then the scaling (making sure all the data is on the same scale). Each of these steps is a transformer - an object that we can make by extending some classes. These steps can be literally anything that we can imagine, as long as we meet the expectations of what a transformer needs (fit and transform methods) and we accept and return the data in the correct format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pipe = pipeline.Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\"))\n",
    "    , (\"scaler\", StandardScaler())\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.    ,   0.    ,   3.    , ...,   1.    ,   0.    ,   7.25  ],\n",
       "       [  2.    ,   1.    ,   1.    , ...,   1.    ,   0.    ,  71.2833],\n",
       "       [  3.    ,   1.    ,   3.    , ...,   0.    ,   0.    ,   7.925 ],\n",
       "       ...,\n",
       "       [889.    ,   0.    ,   3.    , ...,   1.    ,   2.    ,  23.45  ],\n",
       "       [890.    ,   1.    ,   1.    , ...,   0.    ,   0.    ,  30.    ],\n",
       "       [891.    ,   0.    ,   3.    , ...,   0.    ,   0.    ,   7.75  ]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/titanic_train.csv\")\n",
    "cat_cols = df.select_dtypes(include=['object']).columns\n",
    "df.drop(cat_cols, axis=1, inplace=True)\n",
    "np.array(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.73010796, -0.78927234,  0.82737724, ...,  0.43279337,\n",
       "        -0.47367361, -0.50244517],\n",
       "       [-1.72622007,  1.2669898 , -1.56610693, ...,  0.43279337,\n",
       "        -0.47367361,  0.78684529],\n",
       "       [-1.72233219,  1.2669898 ,  0.82737724, ..., -0.4745452 ,\n",
       "        -0.47367361, -0.48885426],\n",
       "       ...,\n",
       "       [ 1.72233219, -0.78927234,  0.82737724, ...,  0.43279337,\n",
       "         2.00893337, -0.17626324],\n",
       "       [ 1.72622007,  1.2669898 , -1.56610693, ..., -0.4745452 ,\n",
       "        -0.47367361, -0.04438104],\n",
       "       [ 1.73010796, -0.78927234,  0.82737724, ..., -0.4745452 ,\n",
       "        -0.47367361, -0.49237783]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_after = pipe.fit_transform(df)\n",
    "pd_after"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Do I Care?\n",
    "\n",
    "For the moment, we don't need to care about the details of those steps, but we do want to use those mechanics to do our work. The steps that the pipeline does can be anything we want, so if we have some spatial cleanup, we can make a transformer to do that, and add it to the pipeline!\n",
    "\n",
    "In machine learning later on, we will use variations of these pipelines to load most of the data we use. It is even more seamless there, we load the data, then the pipeline feeds the output directly into the modelling algorithm - that wall of numbers version above is hidden from us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'imputer': SimpleImputer(strategy='median'), 'scaler': StandardScaler()}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.named_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Can Transformers Do?\n",
    "\n",
    "Pretty much anything you want, some common examples include:\n",
    "<ul>\n",
    "<li> Imputing missing values </li>\n",
    "<li> Scaling numerical features </li>\n",
    "<li> Encoding categorical features </li>\n",
    "<li> Extracting features from text </li>\n",
    "<li> Reducing dimensionality </li>\n",
    "</ul>\n",
    "\n",
    "Basically anything we can express in a statement of \"change the data in this way\" can be a transformer.\n",
    "\n",
    "### And How Do They Do It?\n",
    "\n",
    "A transfomer is a pretty simple concept. It has two main methods:\n",
    "<ul>\n",
    "<li> `fit` - This method is used to learn the parameters of the transformation. For example, if we are scaling numerical features, the `fit` method will calculate the mean and standard deviation of each feature. </li>\n",
    "    <ul>\n",
    "    <li> In many cases, the `fit` method does nothing, it just returns 'self', but it is still required to be there. </li>\n",
    "    </ul>\n",
    "<li> `transform` - This method is used to apply the transformation to the data. For example, if we are scaling numerical features, the `transform` method will subtract the mean and divide by the standard deviation. </li>\n",
    "</ul>\n",
    "\n",
    "Each of these methods is automatically called by the sklearn pipeline when we insert them as steps. If we have any configuration, such as the number of features to extract or the method of scaling, we can pass these as arguments to the transformer's `__init__` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, column_name):\n",
    "        self.column_name = column_name\n",
    "    def fit(self, X, y=None):\n",
    "        return self  # The fit method typically does nothing for transformers\n",
    "        # This is mainly used when there is a 'configuration' step that needs to be done before the transformation\n",
    "        # For example, when scaling data with standardization, we'd need the mean and std of the data - that's calculated here.\n",
    "    def transform(self, X):\n",
    "        # Your transformation logic goes here\n",
    "        X_transformed = X.copy()  # Copy the input DataFrame to avoid modifying the original\n",
    "        X_transformed[self.column_name] = X_transformed[self.column_name].apply(lambda x: x * 2)  # Example transformation\n",
    "        return X_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Transformer to a Pipeline\n",
    "\n",
    "Adding a custom transformer to a pipeline is identical to using a premade one - that's one of the big benefits of the interchangeability of objects we have with python, duck  typing, and inheritance - we only need to provide a tiny portion of the functionality that differs, not learn and reimplement the entire thing!\n",
    "\n",
    "We must conform strictly to the input and output format, as well as the required methods, but beyond that we can do whatever we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geopandas and Pipelines\n",
    "\n",
    "Geopandas is built \"on top of\" pandas, adding the ability to handle geospatial data to the already powerful pandas data manipulation library. We can use geopandas dataframes exactly as we would use a regular pandas dataframe, but with the added ability to handle geospatial data. Since geopandas dataframes are just pandas dataframes with some extra functionality, we can use them in sklearn pipelines as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example - Spatial Join Transformer\n",
    "\n",
    "In this example, we can use a spatial join to connect two datasets based on their location. Specifically, we can use the point position of items in our data to connect with the area polygons in a spatial dataset, and get the neighborhood label from that spatial dataset. To make this work, we'll need a few parts:\n",
    "<ul>\n",
    "<li> A geopandas dataframe with the spatial data. This is like a setting or configuration step, so it will be in the constructor. </li>\n",
    "<li> A transformer that can take the 'regular' data we're using, perform the spatial join, and add the result in a new column. </li>\n",
    "</ul>\n",
    "\n",
    "As long as our input and outputs match the sklearn transformer format, we can use this in a pipeline just like any other transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Spatial_Joiner( BaseEstimator, TransformerMixin):\n",
    "    # Still must complete. \n",
    "    def __init__(self, gdf, column_name):\n",
    "        self.gdf = gdf\n",
    "        self.column_name = column_name\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X_transformed = X.copy()\n",
    "        X_transformed = gpd.sjoin(X_transformed, self.gdf, how=\"left\", op=\"intersects\")\n",
    "        return X_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise - Add Distance in a Transformer\n",
    "\n",
    "For this exercise, create a transformer that adds a new column representing the distance between the two locations in the dataframe. The transformer should take the two columns containing the latitude and longitude of the two locations as input and add a new column with the distance between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Add_Distance_Transformer( BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, column_name):\n",
    "        self.column_name = column_name\n",
    "    def fit(self, X, y=None):\n",
    "        return self  \n",
    "    def transform(self, X):\n",
    "        X_transformed = X.copy()  # Copy the input DataFrame to avoid modifying the original\n",
    "        \n",
    "        return X_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geospatial Transformations\n",
    "\n",
    "With geospatial data, particularly for the things you're likely to be doing, these transformers and piplines can be used to make tools to process the data automatically into a format that provides what you need for analysis. We can create a pipeline that takes in raw data and outputs it in some format that we know we want - the steps to do those transformations are the transformers in the pipeline. Once built, we can process any new data with no additional effort by just running it through the pipeline - this means that we'd never do something like manually manipulate data in Excel or something like that, we'd always use the pipeline to do it for us.\n",
    "\n",
    "For your applications, you'll get some data and the format you need it in might vary, or you likely might need more than one format. For example, if you were displaying some data in Tableau or Power BI, you can take some raw data, run it through a pipeline that calculates whatever values need to be displayed, then output that as a datasource for your visualization. The transformer step might do all kinds of stuff like calculate distance, do spatial joins to get region labels, calculate area, etc... This data can then be fed to the visualization tool, and it can make pretty pictures without having to manipulate data there. There can be multiple pipelines (or outputs) that each prepares one central source of data, automatically, for different purposes.\n",
    "\n",
    "If this sounds similar to some ETL stuff you talked about in the database classes, that's because it is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
