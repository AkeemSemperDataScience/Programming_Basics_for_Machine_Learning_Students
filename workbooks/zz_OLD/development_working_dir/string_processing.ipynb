{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "STOPWORDS = [\"a\", \"the\", \"it\", \"i\", \"to\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# String Processing\n",
    "\n",
    "Most of the basic data types are pretty simple and simple and straightforward, however strings are a bit more complex. Strings can contain any character, be any length, be combined or split, and can contain other types of information embedded in them. This makes strings powerful and flexible, but also more complex to work with. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slicing and Stitching\n",
    "\n",
    "Often when working with strings, we want to extract substrings or combine strings together. We've seen this already when printing things, we can combine multiple strings together with the `+` operator. We can also extract substrings using the `[]` operator and treating the string as a list of characters. \n",
    "\n",
    "When dealing with more text, we can leverage an assortment of tools to make our lives easier.\n",
    "\n",
    "### Splitting\n",
    "\n",
    "We can use the split() function to automatically divide a string into a list of substrings. By default, it splits on whitespace, but we can also specify a different character to split on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining\n",
    "\n",
    "We can use the join() function to combine a list of strings into a single string. We specify the character to insert between each string.\n",
    "\n",
    "We can also build some more complex functions to do more complex string manipulation, such as conditionally adding things like spaces to ensure that the output is formatted correctly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Cleanup\n",
    "\n",
    "Most text cleanup that we need to do is conceptually pretty simple - we want to get rid of what isn't useful to us, and leave the parts of text that are useful - normally the parts that contain the meaning. We normally use premade functions that we import to do this for us, but it is simple for us to explore it. \n",
    "\n",
    "#### Character Level Cleanup\n",
    "\n",
    "The most simple cleaning we can do with text is to remove any useless characters. This can include removing punctuation, but can also be filtering out characters from other languages, removing numbers, stripping special characters, getting rid of whitespace, or removing junk like wingdings. For example, characters like an umlaut or accents that are meaningful in other languages may not be meaningful if we are only working with English. \n",
    "\n",
    "We can make a simple character cleanup function to practice this. We can specify characters to remove, referring to them either by typing the character, or by using the code for that character. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_bad_characters(input_string, bad_chars):\n",
    "    \"\"\"Remove bad characters from input string\"\"\"\n",
    "    for char in input_string:\n",
    "        if char in bad_chars:\n",
    "            input_string = input_string.replace(char, \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whitespace Cleanup\n",
    "\n",
    "Whitespace is a special type of character that is used to separate words and sentences. It is invisible, but it is still a character. We can use the `strip()` function to remove whitespace from the beginning and end of a string. In addition to spaces, whitespace can also include tabs, newlines, and other special characters. These can also cause weird errors as a tab and several spaces are different characters, even if they look the same.\n",
    "\n",
    "<b>Note:</b> whitespace can become an easy place to introduce a bug if not careful. For example, \"Akeem\" and \"Akeem \" are different strings, even if we don't think of them as different outside of programming. Removing whitespace is usually good practice, to eliminate the potential for these sorts of errors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Feature Processing\n",
    "\n",
    "When doing data science work, we typically use text as one of the inputs that we will use to do our analysis. Unfortunately, comments, tweets, prose, or other text that we may get generally doesn't come in nice, clean, and easy to use formats. We will need to do some processing to get the text into a format that we can use. \n",
    "\n",
    "When using text as an input, we want to transform it from whatever format it comes in, into something that we can use. Generally, when we are working with text, we will want to do the following:\n",
    "<ul>\n",
    "    <li>Remove punctuation - get rid of all \"non-text\" parts of the incoming strings. </li>\n",
    "    <li>Remove stop words - remove all of the \"meaningless\" words, usually things such as \"a\" and \"the\", that don't really contribute <i>meaning</i> to the text. </li>\n",
    "    <li>Stemming or lemmatization - reducing text down to root words, such as \"playing\" and \"play\" having a very similar meaning. </li>\n",
    "    <li>Vectorization - transforming the text that we start with into some structured representation that predictive models can process. </li>\n",
    "</ul>\n",
    "\n",
    "We'll look at the first two concepts here, and the final two later on when we move into machine learning stuff. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Create a class that reads in a string of text and stores it in an instance variable in its raw form. Then create the following methods:\n",
    "<ul>\n",
    "    <li> Process - this method should return the text as a string after it has has its punctuation removed and its stop words removed.  </li>\n",
    "    <li> wordCount - this method should return the number of words in the text after processing. </li>\n",
    "    <li> __str__ - return the raw text in a nice format for printing. </li>\n",
    "</ul>\n",
    "\n",
    "<b>Note:</b> You can use the variable at the top of the notebook for a list of stopwords we want to remove, modify it if you want to play around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myText():\n",
    "    \n",
    "    def __init__(self, text) -> None:\n",
    "        pass\n",
    "    \n",
    "    def process(self) -> str:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strings as Tokens\n",
    "\n",
    "When processing natural text data, we will generally want to break the text up into individual* words, called <i>tokens</i>. \n",
    "\n",
    "### Tokens\n",
    "\n",
    "Tokens are the individual words that make up a piece of text. Or, more specifically, tokens are the individual <i>terms</i> that make up a piece of text - meaning that a token can be a word, or it can be a \"term\" that is made up of multiple words, such as \"Information Technology\", \"New York\", or \"deep fried\" that have a meaning that is different from the component words.\n",
    "\n",
    "Our processing aims to transform a string of text into a list of tokens, sometimes called a <i>tokenized</i> string. This format is something that the models that we create later on can use as an input. Raw text is useless for analysis, but that text is useful if we can transform it into a list of tokens.\n",
    "\n",
    "### Do it for the NGram\n",
    "\n",
    "When tokens are allowed to be multiple words long, they are called <i>ngrams</i>. The \"n\" in ngram is a variable that represents the number of words that make up the token. So, a 1gram is a single word, a 2gram is a token that is made up of two words, and so on. When processing text, the n value is something that we need to decide on based on what we are trying to do and what we have for input text. Allowing only small ngrams means that we will be able to process the data more quickly but we may miss out on some meaning that is only conferred by larger ngrams. Allowing larger ngrams means that we will be able to capture more meaning, but we will have more data to process and our models will take longer to run - when text gets very large, this can make a real difference as the raw number of ngrams generated from the text can grow exponentially as we allow n to increase. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Expand on the class above, adding the following methods:\n",
    "<ul>\n",
    "    <li> Tokenize - this method should return a list of tokens, with the punctuation and stop words removed. If you are done, add an argument for ngram size.</li>\n",
    "    <li> Count - return a dictionary of the counts of each token in the text. If you are done, add an argument for \"token\", and have the function return the count for only that token if it is specified. </li>\n",
    "    <li> Unique - return a list of the unique tokens in the text. </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myText():\n",
    "    \n",
    "    def __init__(self, text) -> None:\n",
    "        pass\n",
    "\n",
    "    def tokenize(self) -> list:\n",
    "        pass\n",
    "    \n",
    "    def process(self) -> str:\n",
    "        pass\n",
    "\n",
    "    def count(self) -> dict:\n",
    "        pass\n",
    "\n",
    "    def unique(self) -> list:\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
