{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Management 2\n",
    "\n",
    "## Pandas File Access\n",
    "\n",
    "We've already looked at our main tool for reading data from disk, the file read/write functionality in Pandas. When reading datasets this is normally all we need. Inside the read_csv function the Pandas people have either created all the file access stuff that they need to read a CSV or, more likely, they repurposed and extended some os library functions to do the work for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_id</th>\n",
       "      <th>quantity</th>\n",
       "      <th>item_name</th>\n",
       "      <th>choice_description</th>\n",
       "      <th>item_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Chips and Fresh Tomato Salsa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>$2.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Izze</td>\n",
       "      <td>[Clementine]</td>\n",
       "      <td>$3.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Nantucket Nectar</td>\n",
       "      <td>[Apple]</td>\n",
       "      <td>$3.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Chips and Tomatillo-Green Chili Salsa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>$2.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Chicken Bowl</td>\n",
       "      <td>[Tomatillo-Red Chili Salsa (Hot), [Black Beans...</td>\n",
       "      <td>$16.98</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   order_id  quantity                              item_name  \\\n",
       "0         1         1           Chips and Fresh Tomato Salsa   \n",
       "1         1         1                                   Izze   \n",
       "2         1         1                       Nantucket Nectar   \n",
       "3         1         1  Chips and Tomatillo-Green Chili Salsa   \n",
       "4         2         2                           Chicken Bowl   \n",
       "\n",
       "                                  choice_description item_price  \n",
       "0                                                NaN     $2.39   \n",
       "1                                       [Clementine]     $3.39   \n",
       "2                                            [Apple]     $3.39   \n",
       "3                                                NaN     $2.39   \n",
       "4  [Tomatillo-Red Chili Salsa (Hot), [Black Beans...    $16.98   "
      ]
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"chipotle.tsv\", sep=\"\\t\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OS and File Read/Write\n",
    "\n",
    "We can also use the `os` module to do some basic file management. OS is a library that allows us to interact with the operating system on our local computer. Recall that our Python programs run inside an environment setup by the Python install on our computer. This means that as we work we are \"inside\" that separate environment and we can't directly interact with the underlying computer. The os library, and functions such as read_csv from other libraries, are a tool that allows us to bridge this gap. The os library that is presented to us gives us an assortment of commands to do things like delete files or change the directory we're using. The library's functions are then translated into the correct actions for the actual computer, and passed on to that computer, when the code is run in the Python environment. This also allows for Python code to be portable, or able to run with few to no changes, on different types of computers - I am using a Mac, most of you are probably using a PC, and we can also use a Unix/Linux based system like Google Colab. The code we write can work on all of those environments because of this abstraction, and in each case, the actions triggered by the os module will be different depending on the underlying operating system of the machine. In practice, many of the user-friendly libraries that we might use to access files or folders is built on top of the os module, so we often avoid needing to get into the weeds ourselves.\n",
    "\n",
    "We can use the `os` module to do things like:\n",
    "<ul>\n",
    "    <li>Get the current working directory</li>\n",
    "    <li>Change the current working directory</li>\n",
    "    <li>Get a list of files in a directory</li>\n",
    "    <li>Create a new directory</li>\n",
    "    <li>Remove a directory</li>\n",
    "    <li>Remove a file</li>\n",
    "</ul>\n",
    "\n",
    "First though, let's get some info about our system. Everyone will get totally different results here - I'm on a MacBook Air running MacOS, and I assume most of you have some variety of a PC running Windows. If you've ever seen any of those website things that tells you, \"You are running Windows 10 in Edmonton, Ab...\" this is a similar idea. The os.uname() function reaches out to the computer and retrieves some of it's identifying information for us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Darwin\n",
      "Akeems-MacBook-Air.local\n",
      "22.5.0\n",
      "Darwin Kernel Version 22.5.0: Mon Apr 24 20:52:43 PDT 2023; root:xnu-8796.121.2~5/RELEASE_ARM64_T8112\n",
      "arm64\n"
     ]
    }
   ],
   "source": [
    "for a in os.uname():\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get much more info by looking at the environment variables. These are variables that are set in the operating system and are available to any program running on the computer. These admittedly not frequently relevant to us doing some data science work, but it is a good example of connecting to the system. One thing that can matter to us is the PATH variable, which (roughly) lists all the places the system will look for programs to run. This is how the system knows where to find Python, for example - its in one of those places - as are all the programs installed on your machine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "environ{'COMMAND_MODE': 'unix2003',\n",
       "        'CONDA_DEFAULT_ENV': 'ml_env',\n",
       "        'CONDA_EXE': '/Users/akeem/anaconda3/bin/conda',\n",
       "        'CONDA_PREFIX': '/Users/akeem/anaconda3/envs/ml_env',\n",
       "        'CONDA_PROMPT_MODIFIER': '(ml_env) ',\n",
       "        'CONDA_PYTHON_EXE': '/Users/akeem/anaconda3/bin/python',\n",
       "        'CONDA_SHLVL': '2',\n",
       "        'HOME': '/Users/akeem',\n",
       "        'LOGNAME': 'akeem',\n",
       "        'MallocNanoZone': '0',\n",
       "        'OLDPWD': '/',\n",
       "        'ORIGINAL_XDG_CURRENT_DESKTOP': 'undefined',\n",
       "        'PATH': '/Users/akeem/anaconda3/envs/ml_env/bin:/Users/akeem/anaconda3/condabin:/usr/local/bin:/System/Cryptexes/App/usr/bin:/usr/bin:/bin:/usr/sbin:/sbin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/local/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/appleinternal/bin',\n",
       "        'PWD': '/',\n",
       "        'SHELL': '/bin/zsh',\n",
       "        'SHLVL': '2',\n",
       "        'SSH_AUTH_SOCK': '/private/tmp/com.apple.launchd.UgUjJxLXDo/Listeners',\n",
       "        'TMPDIR': '/var/folders/p1/m8wtcgx57417hx9d_r110ctw0000gn/T/',\n",
       "        'USER': 'akeem',\n",
       "        'VSCODE_AMD_ENTRYPOINT': 'vs/workbench/api/node/extensionHostProcess',\n",
       "        'VSCODE_CODE_CACHE_PATH': '/Users/akeem/Library/Application Support/Code/CachedData/abd2f3db4bdb28f9e95536dfa84d8479f1eb312d',\n",
       "        'VSCODE_CRASH_REPORTER_PROCESS_TYPE': 'extensionHost',\n",
       "        'VSCODE_CWD': '/',\n",
       "        'VSCODE_HANDLES_UNCAUGHT_ERRORS': 'true',\n",
       "        'VSCODE_IPC_HOOK': '/Users/akeem/Library/Application Support/Code/1.82-main.sock',\n",
       "        'VSCODE_NLS_CONFIG': '{\"locale\":\"en-us\",\"osLocale\":\"en-ca\",\"availableLanguages\":{},\"_languagePackSupport\":true}',\n",
       "        'VSCODE_PID': '94507',\n",
       "        'XPC_FLAGS': '0x0',\n",
       "        'XPC_SERVICE_NAME': '0',\n",
       "        '_': '/Users/akeem/anaconda3/envs/ml_env/bin/python',\n",
       "        '__CFBundleIdentifier': 'com.microsoft.VSCode',\n",
       "        '__CF_USER_TEXT_ENCODING': '0x1F5:0x0:0x52',\n",
       "        'ELECTRON_RUN_AS_NODE': '1',\n",
       "        'VSCODE_L10N_BUNDLE_LOCATION': '',\n",
       "        'PYTHONUNBUFFERED': '1',\n",
       "        'PYTHONIOENCODING': 'utf-8',\n",
       "        '_CE_CONDA': '',\n",
       "        'CONDA_PREFIX_1': '/Users/akeem/anaconda3',\n",
       "        'CONDA_ROOT': '/Users/akeem/anaconda3',\n",
       "        '_CE_M': '',\n",
       "        'LC_CTYPE': 'UTF-8',\n",
       "        'PYDEVD_IPYTHON_COMPATIBLE_DEBUGGING': '1',\n",
       "        'PYDEVD_USE_FRAME_EVAL': 'NO',\n",
       "        'TERM': 'xterm-color',\n",
       "        'CLICOLOR': '1',\n",
       "        'FORCE_COLOR': '1',\n",
       "        'CLICOLOR_FORCE': '1',\n",
       "        'PAGER': 'cat',\n",
       "        'GIT_PAGER': 'cat',\n",
       "        'MPLBACKEND': 'module://matplotlib_inline.backend_inline'}"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/akeem/anaconda3/envs/ml_env/bin',\n",
       " '/Users/akeem/anaconda3/condabin',\n",
       " '/usr/local/bin',\n",
       " '/System/Cryptexes/App/usr/bin',\n",
       " '/usr/bin',\n",
       " '/bin',\n",
       " '/usr/sbin',\n",
       " '/sbin',\n",
       " '/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/local/bin',\n",
       " '/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/bin',\n",
       " '/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/appleinternal/bin']"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"PATH\"].split(\":\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shutil module is a partner to the os one, and provides some other functions. Among those, we can see disk usage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "usage(total=994662584320, used=184403357696, free=810259226624)"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "shutil.disk_usage(\"/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Folder Management\n",
    "\n",
    "The OS library also provides for an assortment of folder management functions. We can use these to create, rename, and delete folders.\n",
    "\n",
    "When using large datasets it is very common to have our data distributed over several folders. For example, if we are doing image recognition we might have a folder for \"dogs\", another with \"cars\", and another with \"rutabagas\". To construct our dataset we need to navigate over all of these folders and read in the files, using os and/or some similar libraries. \n",
    "\n",
    "Some common folder actions are:\n",
    "<ul>\n",
    "<li> os.mkdir() - create a new folder </li>\n",
    "<li> os.rename() - rename a folder </li>\n",
    "<li> os.rmdir() - remove a folder </li>\n",
    "<li> os.getcwd() - get the current working directory </li>\n",
    "<li> os.chdir() - change the current working directory </li>\n",
    "<li> os.listdir() - list the contents of a directory </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/akeem/Documents/GitHub/Programming_Basics_for_ML/workbooks/development_working_dir'"
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Note:</b> when using these functions, you need to be careful about where you are in the file system. In particular, the folder location doesn't reset itself automatically if you rerun everything. We need to restart the environment to reset the working directory, or navigate ourselves back to the correct location. The above command got the current working directory back from the Python environment, which in turn got it from the operating system. If we make a change to that directory, then rerun the above cell, we aren't \"reset\" to the original where we were when we started the program. To do that, we'd need to restart the environment, which would kill this current world in which our program is running, and generate a brand new one. \n",
    "\n",
    "Here I'm going to capture the current folder name and then move one level up, and back down. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "development_working_dir\n"
     ]
    }
   ],
   "source": [
    "org_fold = os.getcwd()\n",
    "tmp = os.getcwd().split(\"/\")[-1]\n",
    "print(tmp)\n",
    "os.chdir(\"../\")\n",
    "os.getcwd()\n",
    "os.chdir(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling File System Data\n",
    "\n",
    "We can also capture some of this information, and use it as a variable in our code that we can use to navigate the file system. For example, when grabbing the working directory above we stored it as a string and used the split command to break it into a hierarchy of folder names. Below we can pull the files in a folder and that data is returned in a list. If we have code where we need to jump around from folder to folder, we can use this list to help us navigate. For example, if we have a folder structure like this:\n",
    "\n",
    "```\n",
    "data\n",
    "    - dogs\n",
    "        - dog1.jpg\n",
    "        - dog2.jpg\n",
    "        - dog3.jpg\n",
    "    - cats\n",
    "        - cat1.jpg\n",
    "        - cat2.jpg\n",
    "        - cat3.jpg\n",
    "    - rutabagas\n",
    "        - rutabaga1.jpg\n",
    "        - rutabaga2.jpg\n",
    "        - rutabaga3.jpg\n",
    "```\n",
    "Our \"base\" folder is the data folder, and the subfolders inside are where we'll likely need to do all of our work. We can keep this map of the folder structure in some data structure, then we can use that info to move around. For example, we can find out where we are, visit each subfolder to do some work, and then move back to the base folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of flies: 38\n",
      "\n",
      "010_class_comments.ipynb\n",
      "Clothing.csv\n",
      "file_list\n",
      "005_file_access.ipynb\n",
      "006_objects.ipynb\n"
     ]
    }
   ],
   "source": [
    "# Capture file list\n",
    "# Print 5\n",
    "#<b>Bonus:</b> at some point when we looked at strings, someone asked about putting a newline in an f string. I was wrong, it can work like this, no workarounds needed. \n",
    "file_list = os.listdir()\n",
    "print(f\"Number of flies: {len(file_list)}\\n\")\n",
    "for file in range(5):\n",
    "    print(file_list[file])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Your Walk On\n",
    "\n",
    "![Walk](../../images/walk.gif \"Walk\")\n",
    "![Walk](../images/walk.gif \"Walk\")\n",
    "\n",
    "Manually navigating folder structures is pretty useful, but it also sucks and is error prone. The os.walk() function makes this a bit easier for us by doing the \"walking\" from one folder to another seamlessly. \n",
    "\n",
    "The os.walk() function returns a generator object, which is a special type of object that we can use to iterate over the contents of a folder. The generator object is a tuple, with the first element being the current folder, the second element being a list of subfolders, and the third element being a list of files. We can use this to iterate over the contents of a folder, and then iterate over the contents of each subfolder, and so on.\n",
    "\n",
    "The walk function returns a 3 part tuple for us:\n",
    "<ul>\n",
    "    <li> The current folder </li>\n",
    "    <li> A list of subfolders </li>\n",
    "    <li> A list of files </li>\n",
    "</ul>\n",
    "\n",
    "We can also use a flag to set if our walker will go top down or bottom up. The default is top down, which means we start at the top level folder and work our way down to the files. If we set the flag to False, we start at the bottom and work our way up. In practice, we pertty much always want top down - we have a folder inside our repository, such as \"data\" or \"logs\", and we want to navigate into it. Going the other direction moves \"up\" and if we leave our repository, that's the computer at large, and it could be anything. This may be useful if you're making an installed application, but most things that we're working on are contained to a specific location, so drilling down is normal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "walker = os.walk(\"../\")\n",
    "moonwalker = os.walk(\"../\", topdown=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root: ../\n",
      "Directories: ['file_list', 'development_working_dir', '__pycache__']\n",
      "Files: ['010_class_comments.ipynb', '011_looping_2_data.ipynb', '005_file_access.ipynb', '007_other_data_structures_full_sol.ipynb', '.DS_Store', '006_objects.ipynb', '012_algorithm_design_sol.ipynb', '002_conditional_statements.ipynb', '005a_File_Loading_example_from_stats.ipynb', '004_functions.ipynb', '009_strings_sol.ipynb', 'COPY-012_algorithm_design copy.ipynb', 'COPY-015_overloading.ipynb', '013_arrays_and_numpy_sol.ipynb', '009_strings.ipynb', '011_looping_2_data_sol.ipynb', '014_map_apply.ipynb', '014_map_apply_sol.ipynb', 'COPY-014_map_apply.ipynb', 'fib.csv', 'file_list.zip', 'helper.py', '012_algorithm_design.ipynb', '013_arrays_and_numpy.ipynb', '000_Setup.ipynb', '001a_python.py', 'chipotle.tsv', '015_overloading_sol.ipynb', '001_programming_basics.ipynb', 'COPY-013_arrays_and_numpy.ipynb', '007_other_data_structures.ipynb', '003_loops.ipynb', '015_overloading.ipynb', '008_recursion.ipynb']\n",
      "Root: ../file_list\n",
      "Directories: []\n",
      "Files: ['010_class_comments.ipynb', '011_looping_2_data.ipynb', '005_file_access.ipynb', '007_other_data_structures_full_sol.ipynb', '.DS_Store']\n",
      "Root: ../development_working_dir\n",
      "Directories: ['file_list']\n",
      "Files: ['010_class_comments.ipynb', 'Clothing.csv', '005_file_access.ipynb', '006_objects.ipynb', 'residential.csv', 'Cycling.csv', '002_conditional_statements.ipynb', 'z00_classes.ipynb', 'Car Electronics.csv', 'overloading_sol.ipynb', '005a_File_Loading_example_from_stats.ipynb', '004_functions.ipynb', '009_strings_sol.ipynb', '016_file_management_2.ipynb', 'COPY-Asn_1_sol.ipynb', '013_arrays_and_numpy_sol.ipynb', 'run_from_command.py', 'inheritance.ipynb', '011_looping_2_data_sol.ipynb', '014_map_apply_sol.ipynb', 'Strength Training.csv', 'fib.csv', 'file_list.zip', 'helper.py', '012_algorithm_design.ipynb', 'multip_inh.ipynb', 'residential2.csv', '000_Setup.ipynb', '001a_python.py', 'chipotle.tsv', 'tmp.ipynb', '001_programming_basics.ipynb', 'arrays_and_numpy_sol.ipynb', 'string_processing.ipynb', '007_other_data_structures.ipynb', '003_loops.ipynb', '008_recursion.ipynb']\n",
      "Root: ../development_working_dir/file_list\n",
      "Directories: ['file_list']\n",
      "Files: ['010_class_comments.ipynb', 'Clothing.csv', '005_file_access.ipynb', '.DS_Store', '006_objects.ipynb']\n",
      "Root: ../development_working_dir/file_list/file_list\n",
      "Directories: []\n",
      "Files: []\n",
      "Root: ../__pycache__\n",
      "Directories: []\n",
      "Files: ['helper.cpython-310.pyc']\n"
     ]
    }
   ],
   "source": [
    "for root, directories, files, in walker:\n",
    "    print(f\"Root: {root}\")\n",
    "    print(f\"Directories: {directories}\")\n",
    "    print(f\"Files: {files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root: ../file_list\n",
      "Directories: []\n",
      "Files: ['010_class_comments.ipynb', '011_looping_2_data.ipynb', '005_file_access.ipynb', '007_other_data_structures_full_sol.ipynb', '.DS_Store']\n",
      "Root: ../development_working_dir/file_list/file_list\n",
      "Directories: []\n",
      "Files: []\n",
      "Root: ../development_working_dir/file_list\n",
      "Directories: ['file_list']\n",
      "Files: ['010_class_comments.ipynb', 'Clothing.csv', '005_file_access.ipynb', '.DS_Store', '006_objects.ipynb']\n",
      "Root: ../development_working_dir\n",
      "Directories: ['file_list']\n",
      "Files: ['010_class_comments.ipynb', 'Clothing.csv', '005_file_access.ipynb', '006_objects.ipynb', 'residential.csv', 'Cycling.csv', '002_conditional_statements.ipynb', 'z00_classes.ipynb', 'Car Electronics.csv', 'overloading_sol.ipynb', '005a_File_Loading_example_from_stats.ipynb', '004_functions.ipynb', '009_strings_sol.ipynb', '016_file_management_2.ipynb', 'COPY-Asn_1_sol.ipynb', '013_arrays_and_numpy_sol.ipynb', 'run_from_command.py', 'inheritance.ipynb', '011_looping_2_data_sol.ipynb', '014_map_apply_sol.ipynb', 'Strength Training.csv', 'fib.csv', 'file_list.zip', 'helper.py', '012_algorithm_design.ipynb', 'multip_inh.ipynb', 'residential2.csv', '000_Setup.ipynb', '001a_python.py', 'chipotle.tsv', 'tmp.ipynb', '001_programming_basics.ipynb', 'arrays_and_numpy_sol.ipynb', 'string_processing.ipynb', '007_other_data_structures.ipynb', '003_loops.ipynb', '008_recursion.ipynb']\n",
      "Root: ../__pycache__\n",
      "Directories: []\n",
      "Files: ['helper.cpython-310.pyc']\n",
      "Root: ../\n",
      "Directories: ['file_list', 'development_working_dir', '__pycache__']\n",
      "Files: ['010_class_comments.ipynb', '011_looping_2_data.ipynb', '005_file_access.ipynb', '007_other_data_structures_full_sol.ipynb', '.DS_Store', '006_objects.ipynb', '012_algorithm_design_sol.ipynb', '002_conditional_statements.ipynb', '005a_File_Loading_example_from_stats.ipynb', '004_functions.ipynb', '009_strings_sol.ipynb', 'COPY-012_algorithm_design copy.ipynb', 'COPY-015_overloading.ipynb', '013_arrays_and_numpy_sol.ipynb', '009_strings.ipynb', '011_looping_2_data_sol.ipynb', '014_map_apply.ipynb', '014_map_apply_sol.ipynb', 'COPY-014_map_apply.ipynb', 'fib.csv', 'file_list.zip', 'helper.py', '012_algorithm_design.ipynb', '013_arrays_and_numpy.ipynb', '000_Setup.ipynb', '001a_python.py', 'chipotle.tsv', '015_overloading_sol.ipynb', '001_programming_basics.ipynb', 'COPY-013_arrays_and_numpy.ipynb', '007_other_data_structures.ipynb', '003_loops.ipynb', '015_overloading.ipynb', '008_recursion.ipynb']\n"
     ]
    }
   ],
   "source": [
    "for root, directories, files, in moonwalker:\n",
    "    print(f\"Root: {root}\")\n",
    "    print(f\"Directories: {directories}\")\n",
    "    print(f\"Files: {files}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Attempt to list the folders and files in the level above where this notebook is located. Create a list of folders and a list of files. Return them in a tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genList(path=\"../\"):\n",
    "    folders = []\n",
    "    files = []\n",
    "    for root, directories, files in os.walk(path):\n",
    "        #folders.append(root)\n",
    "        folders.append(directories)\n",
    "        files.append(files)\n",
    "    return folders, files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['file_list', 'development_working_dir', '__pycache__'],\n",
       " [],\n",
       " ['file_list'],\n",
       " ['file_list'],\n",
       " [],\n",
       " []]"
      ]
     },
     "execution_count": 449,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genList()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['helper.cpython-310.pyc', [...]]"
      ]
     },
     "execution_count": 450,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genList()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exists\n",
    "\n",
    "We can also check if a file or folder exists. This is useful if we're going to be creating a new file or folder, and we want to make sure we don't overwrite something that already exists. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(os.path.exists(\"chipotle.tsv\"))\n",
    "print(os.path.exists(\"mad_bitcoin_keys.tsv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Paths\n",
    "\n",
    "When navigating file systems we necessarily need to deal with file paths, or the locations in the file structure where each file is located. There are two types of file paths, absolute and relative.\n",
    "\n",
    "![Paths](../../images/paths.png \"Paths\")\n",
    "![Paths](../images/paths.png \"Paths\")\n",
    "\n",
    "#### Absolute Paths\n",
    "\n",
    "An absolute path is the full path to a file or folder. It starts at the root of the file system and lists each folder in the hierarchy, separated by a slash. For example, on my Mac, the root of the file system is \"/\". If I want to get to my home folder, I need to go to \"/Users/\". If I want to get to my Documents folder, I need to go to \"/Users/akeem/Documents\". If I want to get to my \"downloads\" folder, I need to go to \"/Users/akeem/Downloads\".\n",
    "\n",
    "Absolute file paths will rarely be relevant for us, it is relevant if you are writing lower level programs, that deal more directly with a file system. Note that different operating systems have different file systems, so the absolute path on a Mac is different than the absolute path on a PC - you can't rely on any particular configuration. \n",
    "\n",
    "<b>Note:</b> those environment variables that we mentioned above are some of the things that tell programs where to look for stuff, they indicate the root directories of things like the user's home folder, or the location of the program files.\n",
    "\n",
    "#### Relative Paths\n",
    "\n",
    "Relative paths are relative to the current working directory. If I am in the root directory of this repository, the relative path to each workbook is workbooks\\workbook_name.ipynb. If I am in the workbooks folder, the relative path to each workbook is just the name of the workbook. If I am in the data folder, the relative path to each workbook is ../workbooks/workbook_name.ipynb.\n",
    "\n",
    "When working within a repository we use relative paths to refer to other files. This allows us to pass around the repository, or move it to a different machine, and things still work. For example, all the data and image references in these workbooks uses a relative path, so it works just fine on my Mac and your different computers without any issue. One important value when using relative paths is ../ which means \"go up one level\", so if we are in the \"workbooks\" folder here, we can use ../ to get to the root of the repository.\n",
    "\n",
    "### Joining Paths\n",
    "\n",
    "When we need to deal with file paths we can use the os.path.join() function to join together the parts of the path. This is useful because it will automatically handle the differences between operating systems for us. For example, on a Mac the path separator is \"/\", while on a PC it is \"\\\". The os.path.join() function will automatically use the correct separator for the operating system we are using. \n",
    "\n",
    "For example, to get the absolute path to a file, we can use the os.path.join() function to join together the current working directory and the relative path to the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File path: /Users/akeem/Documents/GitHub/Programming_Basics_for_ML/workbooks/development_working_dir/016_file_management_2.ipynb\n"
     ]
    }
   ],
   "source": [
    "file_name = \"016_file_management_2.ipynb\"\n",
    "fold_path = os.getcwd()\n",
    "print(f\"File path: {os.path.join(fold_path, file_name)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and Writing Text Files\n",
    "\n",
    "Now that we have an idea what is in our folders, we can start recklessly changing things. For example, we can use the os write functions to make a new CSV file and write some data to it. One thing that is reveled to us here that might not be visible is you're used to Windows machines is a look at what is a \"text\" file. Text files are .txt, but also .csv, .tsv, .py, etc... meaning all of these types of files is made up of just plain text, and we can edit them in any text editor on a computer - the file extension doesn't dictate it, that's for our convenience. The structure of our code will:\n",
    "<ul>\n",
    "<li> Open a connection to the file, if it doesn't exist this will create it. </li>\n",
    "<li> Perform the contents of the loop - writing some data to a file for 100 lines. </li>\n",
    "<li> When that writing task is complete, it will close the connection automatically thanks to the with. </li>\n",
    "</ul>\n",
    "\n",
    "In the open() function call to connect to the file we provide the second argument that defines what type of access we get to the file we are opening:\n",
    "<ul>\n",
    "<b><li> 'r' - read only </li>\n",
    "<li> 'w' - write only </li>\n",
    "<li> 'a' - append to the end of the file </li>\n",
    "<li> 'r+' - read and write </li></b>\n",
    "<li> 'w+' - write and read, overwrites existing files</li>\n",
    "</ul>\n",
    "\n",
    "These are mostly pretty simple and self-explanatory, with the exception of the distinction between r+ and w. The 'r+' option is a bit more complicated. This will open the file for reading and writing, but it will not create the file if it doesn't exist. If you try to open a file that doesn't exist with 'r+' you will get an error. The `w` option will create the file if it doesn't exist, but it will also overwrite the file if it does exist. So if we are making something brand new, we want `w`, but if we are attempting to update an existing file, we want `r+`. This is an easy place to make an error, so we should be careful. We can also check to see if the file we want to make already exists, then make a decision. `W+` is another weird option, it is read/write, but will overwrite the file if it exists.\n",
    "\n",
    "![File Permissions](../../images/file_permissions.png \"File Permissions\" )\n",
    "![File Permissions](../images/file_permissions.png \"File Permissions\" )\n",
    "\n",
    "Choosing the level of access of a file that we open is important in terms of writing our code to prevent errors. We want to open the file with the least impactful level of access that we need to have to do what we want. So if we are just reading data from a file, opening it as read only will prevent us accidentally changing that file in any way, as we don't even have the ability to do so. If we want a brand-new file, opening it as write only will prevent any old data that may have been hanging around from persisting. The more flexible the level of access, the more options we have for what we can do, the more likely it is that we may do something unintentional.\n",
    "\n",
    "<b>Note:</b> this will go to our current directory, wherever the pointer is, so if you got rid of that line to reset the locations, it would spit this file out to whatever folder you happen to be in.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a CSV with Fibonacci Numbers\n",
    "\n",
    "Here we'll make a CSV, fib.csv, that has two coulmns - an index and the fibonacci number at that index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# write fibonacci series to a file\n",
    "end_fib = 30\n",
    "\n",
    "def fib(n):\n",
    "    if n < 2:\n",
    "        return n\n",
    "    else:\n",
    "        return fib(n-2) + fib(n-1)\n",
    "\n",
    "with open(\"fib.csv\", \"w\") as f:\n",
    "    f.write(\"Index,Fibonacci Series\\n\")\n",
    "    i = 0\n",
    "    while i < end_fib:\n",
    "        f.write(f\"{i},{fib(i)}\\n\")\n",
    "        #print(f\"{i},{fib(i)}\")\n",
    "        i += 1\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading Our File\n",
    "\n",
    "Now that our file is written, we can read it and see what we got. We need to specify the `r` here, since we are only reading. When reading from a file, there's a few main options:\n",
    "<ul>\n",
    "<li> read() - read the entire file into a single string </li>\n",
    "<li> readline() - read the file one line at a time </li>\n",
    "<li> readlines() - read the file into a list of strings, one per line </li>\n",
    "</ul>\n",
    "\n",
    "The first, read, takes in the entire file into one string. This is fine for small files, but for things that are large it is unruly. For most things of size we probably want to navigate the file one line at a time, using the readline() option. There's also a common shortcut that we can do with a for loop that does this for us easily:\n",
    "    \n",
    "    ``` for line in file: ```   \n",
    "\n",
    "If we are reading in a large file, one line at a time is a better choice. Depending on what we are doing, we may be able to process our data and \"deal with it\" - whether that be saving it to another file or loading it into some dataset - on the fly. When we get to neural networks towards the end of machine learning, we'll try to read enough data so that our processor is busy - so the computer is never waiting for either data or a free processor. Loading batches allows us to make the most of the power of our computer, as we can minimize the amount of time any part of it spends waiting for something else to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index,Fibonacci Series\n",
      "\n",
      "0,0\n",
      "\n",
      "1,1\n",
      "\n",
      "2,1\n",
      "\n",
      "3,2\n",
      "\n",
      "4,3\n",
      "\n",
      "5,5\n",
      "\n",
      "6,8\n",
      "\n",
      "7,13\n",
      "\n",
      "8,21\n",
      "\n",
      "9,34\n",
      "\n",
      "10,55\n",
      "\n",
      "11,89\n",
      "\n",
      "12,144\n",
      "\n",
      "13,233\n",
      "\n",
      "14,377\n",
      "\n",
      "15,610\n",
      "\n",
      "16,987\n",
      "\n",
      "17,1597\n",
      "\n",
      "18,2584\n",
      "\n",
      "19,4181\n",
      "\n",
      "20,6765\n",
      "\n",
      "21,10946\n",
      "\n",
      "22,17711\n",
      "\n",
      "23,28657\n",
      "\n",
      "24,46368\n",
      "\n",
      "25,75025\n",
      "\n",
      "26,121393\n",
      "\n",
      "27,196418\n",
      "\n",
      "28,317811\n",
      "\n",
      "29,514229\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"fib.csv\", \"r\") as f:\n",
    "   for line in f:\n",
    "      print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Index,Fibonacci Series\\n', '0,0\\n', '1,1\\n', '2,1\\n', '3,2\\n', '4,3\\n', '5,5\\n', '6,8\\n', '7,13\\n', '8,21\\n', '9,34\\n', '10,55\\n', '11,89\\n', '12,144\\n', '13,233\\n', '14,377\\n', '15,610\\n', '16,987\\n', '17,1597\\n', '18,2584\\n', '19,4181\\n', '20,6765\\n', '21,10946\\n', '22,17711\\n', '23,28657\\n', '24,46368\\n', '25,75025\\n', '26,121393\\n', '27,196418\\n', '28,317811\\n', '29,514229\\n']\n"
     ]
    }
   ],
   "source": [
    "with open(\"fib.csv\", \"r\") as f:\n",
    "    fib_list = f.readlines()\n",
    "print(fib_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also read it into a CSV, for fun!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>Fibonacci Series</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Index  Fibonacci Series\n",
       "0      0                 0\n",
       "1      1                 1\n",
       "2      2                 1\n",
       "3      3                 2\n",
       "4      4                 3"
      ]
     },
     "execution_count": 456,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.read_csv(\"fib.csv\", index_col=None)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seek\n",
    "\n",
    "When in a file, we can use the seek() function to move around. This is useful if we want to go back to the beginning of the file, or jump to a specific location. The seek() function takes in two arguments, the first is the location we want to go to, and the second is the reference point. The reference point can be 0, 1, or 2.\n",
    "<ul>\n",
    "<li>0 is the beginning of the file</li>\n",
    "<li>1 is the current location</li>\n",
    "<li>2 is the end of the file</li>\n",
    "</ul>\n",
    "\n",
    "So if we want to go to the beginning of the file, we can use seek(0,0). If we want to go to the end of the file, we can use seek(0,2). If we want to go to the 100th character in the file, we can use seek(100,0). If we want to go to the 100th character from the end of the file, we can use seek(-100,2).\n",
    "\n",
    "Note that in the examples below I've changed a couple of things - the open uses \"rb\" or \"read binary\" instead of \"r\" or \"read\". This is to allow us to do the offsets from the end or the current position - the system needs to deal with the binary file. Second, I've added that decode statement to the print statement. This is because the file is binary, so we need to decode it to get it into a string that we can print nicely. Note the last one - I didn't change the encoding. Text encoding is something we need to deal with, but something that is minor - we basically need to make sure that we are using the encoding that matches the file, or we need to re-encode it. \n",
    "\n",
    "#### Tell\n",
    "\n",
    "We can also use the tell() function to find out where we are in the file. This is useful if we want to jump around, but we don't know where we are. We can use tell() to find out where we are, then use seek() to go somewhere else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At the stat of the file:\n",
      "Position 0\n",
      "Index,Fibonacci Series\n",
      "\n",
      "At position 150:\n",
      "Position 150\n",
      "946\n",
      "\n",
      "At position 20, relative to the current position of 150:\n",
      "Position 174\n",
      ",46368\n",
      "\n",
      "At position 5:\n",
      "Position 5\n",
      ",Fibonacci Series\n",
      "\n",
      "At the end:\n",
      "Position 230\n",
      "\n",
      "Moving back from the end:\n",
      "Position 225\n",
      "b'4229\\n'\n"
     ]
    }
   ],
   "source": [
    "# Open fib and seek/tell\n",
    "with open(\"fib.csv\", \"rb\") as f:\n",
    "    print(\"At the stat of the file:\")\n",
    "    print(\"Position\", f.tell())\n",
    "    print(f.readline().decode(\"utf-8\"))\n",
    "\n",
    "    print(\"At position 150:\")\n",
    "    f.seek(150, 0)\n",
    "    print(\"Position\", f.tell())\n",
    "    print(f.readline().decode(\"utf-8\"))\n",
    "\n",
    "    print(\"At position 20, relative to the current position of 150:\")\n",
    "    f.seek(20, os.SEEK_CUR)\n",
    "    print(\"Position\", f.tell())\n",
    "    print(f.readline().decode(\"utf-8\"))\n",
    "\n",
    "    print(\"At position 5:\")\n",
    "    f.seek(5, 0)\n",
    "    print(\"Position\", f.tell())\n",
    "    print(f.readline().decode(\"utf-8\"))\n",
    "\n",
    "    print(\"At the end:\")\n",
    "    f.seek(0, os.SEEK_END)\n",
    "    print(\"Position\", f.tell())\n",
    "    print(f.readline().decode(\"utf-8\"))\n",
    "\n",
    "    print(\"Moving back from the end:\")\n",
    "    f.seek(-5, 2)\n",
    "    print(\"Position\", f.tell())\n",
    "    print(f.readline())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Appending a File and the In-File Pointer\n",
    "\n",
    "Another of the options above that is a little odd is the `a`, for append. This will open the file and add new data to the end of it, but it will not overwrite the existing data. This is useful if we want to add new data to an existing file, but we don't want to lose the old data. This is very useful for things like logs - we likely have a pretty substantial amount of data accumulated in a big text file and we want to add new stuff without losing the old data or having to deal with the old data at all. We can use this to basically tack some entries onto the end of a file easily.\n",
    "\n",
    "This issue of opening an existing file normally vs appending seems pretty minor, but it can have larger performance implications than we might expect. For example, server logs can be many, many GB of text that lists errors or warnings going back years. We want to keep the log, and we also want to add today's entries. Opening a 2GB file \"normally\", navigating to the end, then spitting it back out can be slow, appending directly to the end of that same file is fast. This is because just like navigating a file system, a text file itself has a pointer that maintains your position - think of it as an invisible cursor just like we have in any program where we type. Append puts that position cursor directly at the end, and just starts writing. Personally, I once had a job where I remade a little program that went to approximately 150 servers, grabbed their log file, and looked for last night's entry at the end of the file to see if a backup failed. By changing it from opening the files normally, to appending (roughly, the language wasn't Python), I cut the runtime from 4 to 5 hours to about <10 minutes - without making any actual improvements to the logic of the code, just by jumping directly to the end of the text. When someone wrote the original, all the log files were probably tiny, as the system was new, so it didn't matter for performance; as things grew, this became an issue.  \n",
    "\n",
    "<b>Note:</b> the \"it's slow to open a large file and write to the end\" thing is obviously a common issue for computers in general. File access packages know this, and are built to be fast no matter what. This idea is still true, just less true than it is with older software."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230\n"
     ]
    }
   ],
   "source": [
    "with open(\"fib.csv\", \"a\") as f:\n",
    "    print(f.tell())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Complete the spotReader function to take in a file and a spot in that file, and return that line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spotReader(filepath, spot):\n",
    "    ret_val = \"\"\n",
    "    with open(filepath, \"r\") as f:\n",
    "        f.seek(spot, 0)\n",
    "        ret_val = f.readline()\n",
    "    return ret_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'946\\n'"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spotReader(\"fib.csv\", 150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remote Data\n",
    "\n",
    "We can also use some code to programmatically download data from the internet. This can save us from having to download large files, but it can also help us to build automated pipelines for getting data. \n",
    "\n",
    "One thing that I've worked on a lot in industry is importing data from other systems into LMS systems like Moodle. A common process to do this is for the other system to export a CSV file to a specific location on a file server, then a script that we created will grab the new file from the pre-defined location and feed it into our import work. Applications for personal use are also broad - we could automate downloads of files that are regularly updated. \n",
    "\n",
    "For pretty much any data source that we might want to be able to access, there is likely a library that will do so. So we can access FTP servers, different file share protocols, and so on - we just need to look up the correct tool for whichever data source we want to access.\n",
    "\n",
    "<b>Note:</b> there are many libraries that download files, they're pretty much interchangeable. I'm using `urllib.request` here because it's built into Python and is the \"basic standard\", but you could also use `requests` or `wget` or `curl` or any number of other libraries. For this, and the others, look at the documentation to see the options and how to use the functions - they are generally similar to this, just provide a URL and a destination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download\n",
    "url = 'https://raw.githubusercontent.com/justmarkham/DAT8/master/data/chipotle.tsv'\n",
    "import urllib.request\n",
    "try:\n",
    "    urllib.request.urlretrieve(url, 'chipotle.tsv')\n",
    "except:\n",
    "    print(\"Error downloading file\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compression\n",
    "\n",
    "Many files that we deal with, particularly when downloading datasets, may be compressed. Several libraries provide tools for us to programmatically deal with these files, including decompressing them and moving their files into our working directory.\n",
    "\n",
    "<b>Note:</b> Pandas read_csv function can also read compressed files directly, so you can load my_data.zip or similar with no interim decompression step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compress some files\n",
    "import zipfile\n",
    "\n",
    "# Zip first 5 files in file_list\n",
    "with zipfile.ZipFile('file_list.zip', 'w') as myzip:\n",
    "    for file in range(5):\n",
    "        myzip.write(file_list[file])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decompress into a folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decompress file_list.zip into file_list folder\n",
    "with zipfile.ZipFile('file_list.zip', 'r') as myzip:\n",
    "    myzip.extractall('file_list')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Create the function ipynb crawler. When given a director, assemble and return a list of all .ipynb files in that directory and all subdirectories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ipynbCrawler(directory_targ=\"../\"):\n",
    "    \"\"\"\n",
    "    Crawl a directory and return a list of all ipynb files\n",
    "    \"\"\"\n",
    "    ipynb_list = []\n",
    "    for root, directories, files in os.walk(directory_targ):\n",
    "        for file in files:\n",
    "            if file.endswith(\".ipynb\"):\n",
    "                ipynb_list.append(os.path.join(root, file))\n",
    "    return ipynb_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../010_class_comments.ipynb',\n",
       " '../011_looping_2_data.ipynb',\n",
       " '../005_file_access.ipynb',\n",
       " '../007_other_data_structures_full_sol.ipynb',\n",
       " '../006_objects.ipynb',\n",
       " '../012_algorithm_design_sol.ipynb',\n",
       " '../002_conditional_statements.ipynb',\n",
       " '../005a_File_Loading_example_from_stats.ipynb',\n",
       " '../004_functions.ipynb',\n",
       " '../009_strings_sol.ipynb',\n",
       " '../COPY-012_algorithm_design copy.ipynb',\n",
       " '../COPY-015_overloading.ipynb',\n",
       " '../013_arrays_and_numpy_sol.ipynb',\n",
       " '../009_strings.ipynb',\n",
       " '../011_looping_2_data_sol.ipynb',\n",
       " '../014_map_apply.ipynb',\n",
       " '../014_map_apply_sol.ipynb',\n",
       " '../COPY-014_map_apply.ipynb',\n",
       " '../012_algorithm_design.ipynb',\n",
       " '../013_arrays_and_numpy.ipynb',\n",
       " '../000_Setup.ipynb',\n",
       " '../015_overloading_sol.ipynb',\n",
       " '../001_programming_basics.ipynb',\n",
       " '../COPY-013_arrays_and_numpy.ipynb',\n",
       " '../007_other_data_structures.ipynb',\n",
       " '../003_loops.ipynb',\n",
       " '../015_overloading.ipynb',\n",
       " '../008_recursion.ipynb',\n",
       " '../file_list/010_class_comments.ipynb',\n",
       " '../file_list/011_looping_2_data.ipynb',\n",
       " '../file_list/005_file_access.ipynb',\n",
       " '../file_list/007_other_data_structures_full_sol.ipynb',\n",
       " '../development_working_dir/010_class_comments.ipynb',\n",
       " '../development_working_dir/005_file_access.ipynb',\n",
       " '../development_working_dir/006_objects.ipynb',\n",
       " '../development_working_dir/002_conditional_statements.ipynb',\n",
       " '../development_working_dir/z00_classes.ipynb',\n",
       " '../development_working_dir/overloading_sol.ipynb',\n",
       " '../development_working_dir/005a_File_Loading_example_from_stats.ipynb',\n",
       " '../development_working_dir/004_functions.ipynb',\n",
       " '../development_working_dir/009_strings_sol.ipynb',\n",
       " '../development_working_dir/016_file_management_2.ipynb',\n",
       " '../development_working_dir/COPY-Asn_1_sol.ipynb',\n",
       " '../development_working_dir/013_arrays_and_numpy_sol.ipynb',\n",
       " '../development_working_dir/inheritance.ipynb',\n",
       " '../development_working_dir/011_looping_2_data_sol.ipynb',\n",
       " '../development_working_dir/014_map_apply_sol.ipynb',\n",
       " '../development_working_dir/012_algorithm_design.ipynb',\n",
       " '../development_working_dir/multip_inh.ipynb',\n",
       " '../development_working_dir/000_Setup.ipynb',\n",
       " '../development_working_dir/tmp.ipynb',\n",
       " '../development_working_dir/001_programming_basics.ipynb',\n",
       " '../development_working_dir/arrays_and_numpy_sol.ipynb',\n",
       " '../development_working_dir/string_processing.ipynb',\n",
       " '../development_working_dir/007_other_data_structures.ipynb',\n",
       " '../development_working_dir/003_loops.ipynb',\n",
       " '../development_working_dir/008_recursion.ipynb',\n",
       " '../development_working_dir/file_list/010_class_comments.ipynb',\n",
       " '../development_working_dir/file_list/005_file_access.ipynb',\n",
       " '../development_working_dir/file_list/006_objects.ipynb']"
      ]
     },
     "execution_count": 465,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ipynbCrawler()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
